# Paper

- Title: Generalization through Memorization: Nearest Neighbor Language Models
- Authors: Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis
- Link: https://arxiv.org/abs/1911.00172
- Repository: https://github.com/urvashik/knnlm
- Tags: Retrieval-augmented generation
- Year: 2020

# Summary

- What
  
  - They introduce the kNN-LM architecture, consisting of a pretrained LM interpolated with a kNN model.
  - The nearest neighbours can be from any text collection, including but not limited to the LM's training data.
  - The space from where the nearest neighbours are drawn is the LM's embedding space.
  - The kNN augmentation is applied at inference and does not require any further training of the LM.

- How

  - Architecture
    
    The system is made of the following components:
    - A pretrained transformer LM.
    - A dataset, which can conicide with the LM's train set or not. For each target token in the dataset, they compute a data store containing the target token as the value and the embedding of its left context (computed via a forward pass through the LM's encoder) as the key.
    - They use the FAISS library for fast nearest neighbour retrieval.
    - An interpolation function that maps the distribution over the retrieved tokens $p_{kNN}$ and the distribution over the candidate decoded tokens $p_{LM}$ to the final probability distribution to give the generated token. That is: $p(y|x) = \lambda p_{kNN} (y|x) + (1 - \lambda) p_{LM} (y|x)$, where $\lambda$ is a hyperparameter tuned on the validation set.
  
  - Strategy
    
    At inference time, when the LM generates a distribution over the vocabulary for the next word given the context, this distribution is augmented as follows:
    - The context encoded by the LM is also used to query the data store and retrieve its k nearest neighbours, together with their values (i.e. the associated target tokens). Note that the authors use $L^2$ distance to measure the similarity between context embeddings.
    - A probability distribution is computed over the k retrieved tokens, by passing the negative distances of their corresponding keys (i.e. contect embeddings) from the input context into a softmax function, and also aggregating the probabilities of any token appearing more than once among the retrievals.
    - The resulting probability distribution and the probability distribution generated by the LM are passed through the interpolation function shown above to obtain the final $p(y|x)$.
  
 ![image](https://user-images.githubusercontent.com/89645136/236224621-9eb059bc-be7a-4569-937b-c6932ac4d481.png)

- Results

  - They show that kNN-LM improves the perplexity score on the Wikitext-103 dataset to a new SOTA.
  - They also show that training a model in Wiki-100M and then using this model to buildm a data store using Wiki-3B and applying the kNN-LM methof on this model and this data store outperforms training a model on the whole Wiki-3B dataset.
  - They show that even using only 1.6B examples from Wiki-3B to make the data store is already enough to beat the performance of the model train on the Wiki-3B dataset.
  - They show that adding a data store from another dataset (e.g. using the Toronto Book Corpus and a model trained on Wiki-3B) dramatically improves the performance of that model on the external dataset which wasn't trained on (e.g. the Book corpus), even though the performance is not better than that of a model trained on that specific dataset.

<img src="https://github.com/lisaalaz/papers/blob/master/images/kNN-LM_domain_adaptation_results.png" width="700">
