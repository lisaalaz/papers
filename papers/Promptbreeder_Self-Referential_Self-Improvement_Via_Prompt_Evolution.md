# Paper

- Title: Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution
- Authors: Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, Tim Rockt√§schel
- Link: https://arxiv.org/abs/2309.16797
- Repository: unavailable
- Tags: LLMs, prompt tuning, reasoning
- Year: 2023

# Summary

- What
  
  - A mechanism to evolve prompts for a given domain via mutation and selection, over multiple generations, as in an evolutionary strategy.
  - The mutation is is governed by mutation-prompts generated by the LLM itself and that are also improved by the LLM over time.

- How

  - They provide the LLM with:
    1) a set of 'mutation-prompts', i.e. instructions to modify a task-prompt
    2) 'thinking-styles', i.e. descrptions of general cognitive heuristics (e.g. "Let's think step by step")
    3) a domain-specific problem description
  - Promptbreeder thus generates variations of both the task-prompts and the mutation-prompts
  - The task-prompts are measured for fitness on a training set, and a subset of successful task-prompts (with their associated generation-prompts) are selected to breed the following generation
  - This process results in the prompt gradually adapting to the given problem domain.

- Mutation

  To mutate prompts, they apply several strategies:

  - Direct mutation (conditions on just one parent):
  
    1) Zero-order Prompt Generation: concatenate the original instruction with the suffix "A list of 100 hints:"
    2) First-order Prompt Generation: concatenate the mutation-prompt and the task prompt

  - Estimation of distribution mutation (conditions on multiple parents):
  
    1) Estimation of Distribution (EDA) Mutation: they provide a filtered (to eliminate too similar prompts) list of the current population of task-prompts to the LLM and ask it to continue this list with new task-prompts
    2) EDA Rank and Index Mutation: same as above but with prompts listed in ascending order of fitness (as LLMs suffer from recency bias, i.e. they are more likely to generate new samples similar to the exemplars that appear later in the list of exemplars)
    3) Lineage Based Mutation: for each generation they store a history of the samples that were the best in the population and provide this (unfiltered) chronological list to the LLM, prefixed with "GENOTYPES FOUND IN ASCENDING ORDER OF QUALITY" to generate a new prompt

  - Hypermutation (mutation of mutation-prompts):

    1) Zero-order Hyper-Mutation: they concatenate the original problem description to a randomly sampled thinking-style, and feed it to the LLM to generate a new mutation-prompt
    2) First-order Hyper-Mutation: they concatenate the hyper-mutation-prompt "Please summarize and improve the following instruction:" to a mutation-prompt so that the LLM generates a new mutation-prompt

  - Lamarckian mutation (generate a new task-prompt - i.e. the genotype - from a working out - i.e. the phenotype)

    1) Working Out to Task-Prompt: give an LLM a previously generated working out that led to a correct answer via the following prompt: "I gave a friend an instruction and some advice. Here are the correct examples of his workings out + $<$correct working out$>$ + The instruction was:"
  
  - Prompt crossover and context shuffling
   
    1) Prompt Crossover: After mutation, a task-prompt is replaced with a randomly chosen task-prompt from another member of the population, with 10% chance. This member is picked via fitness-proportionate selection. Crossover is only applied to task-prompts (not mutation-prompts).
    2) Context Shuffling: to evolve the set of correct working outs (i.e. the few shot exemplars), first the context is filled with working outs that led to correct answers. If the few-shot context list is full, a single randomly sampled new correct working out replaces an existing working out from the list after fitness evaluation of a unit on a new set of questions. Also with a 10% chance they resample the whole context list with probability inverse to the maximum context list length.


- Experimental setup
  
  They used population size of 50, evolved for 20-30 generations. In each generation they form random pairs of samples and compete them against each other. 

- Results

  They evaluate Promptbreeder on:

  1) GSM8K, SVAMP, MultiArith, AddSub, AQuA-RAT, SingleEq (for arithmetic reasoning)
  2) CommonsenseQA, StrategyQA (for commonsense reasoning)
  3) instruction induction tasks 
  4) ETHOS (for speech classification)
   
  Results and baselines are shown in Tab. 1.
