# Paper

- Title: Reasoning with Language Model is Planning with World Model
- Authors: Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu
- Link: https://arxiv.org/abs/2305.14992
- Repository: https://github.com/Ber666/RAP
- Tags: Reasoning, planning, decision making
- Year: 2023

# Summary

- What
  
  - They present a novel reasoning framework for LLMs, called Reasoning via Planning (RAP). In RAP, the LLM acts both as the agent and as the world model.

- How

  - Strategy
    - The LLM agent incrementally builds a reasoning tree while reasoning under the guidance of the LLM world model and task-specific rewards.
    - A planning algorithm based on Monte Carlo Tree search is used to explore the (large) reasoning space.
    - The authors claim that this method allows the agent to obtain a high-reward reasoning path in a way that balances exploration and exploitation.
  
  - Architecture
    
    - __World model:__ a world model predicts the next state after applying an action to the current state. States and actions in RAP depend on the specific task (e.g. in Blocksworld a state is a configuration of blocks and an action is moving a block, whereas in maths reasoning problems a state is the values of intermediate variables and an action is a subquestion that drives the reasoning to compute new values). Note that the LLM world model is instantiated by the same LLM as the LLM agent - the difference between them is in the prompt.
    - __Markov decision process:__ in RAP the reasoning process is an MDP whereby the LLM agent, given the current state $s_t$ (for $t=0,1,...,T$), generates an action $a_t$ following its generative distribution, that is, $a_t ~ p(a|s_t, c)$ where $c$ is a prompt to encourage action generation from the LLM. The world model then predicts the next state $s_{t+1}$ of the reasoning following a distribution $p(s_{t+1}|s_t, a_t, c')$ where $c'$ is another prompt to guide the LLM to generate a state (rather than an action as in the LLM agent). For example, in Blocksworld tasks the LLM world model takes the state description $s_t$ and the action $a_t$ as inputs and generates a new state description $s_{t+1}$.
    - __Reasoning trace:__ a reasoning trace is a sequence of (predicted) states and actions $(s_0, a_0, s_1, ..., a_{T-1}, s_T)$, which is formed as the reasoning progresses. This is different for example from the chain-of-thought setting where the intermediate reasoning is given by intermediate actions only, but not states. The trace is simulated by the world model and used to reason without actually interacting with the environment.
    - __Reward:__ a reward function $r_t = r(s_t, a_t) \in \R$ is necessary for assessing each reasoning step (i.e. applying an action $a_t$ to a state $s_t$). The reward depends on the task, so they design several rewards that are applicable to various tasks, namely: (1) likelyhood of the action -  they use the log probability of the action as it is generated by the LLM, as this indicates LLM preference; (2) confidence of the state - they draw multiple sample answers when they query the LLM world model for a state description, and use the proportion of the most frequent answer as the confidence, they use the confidence of the state in the reward as this indicates a reliable reasoning step; (3) LLM self-evaluation - they prompt the LLM with the question "is this reasoning step correct?" and use the next-word probability of the token "yes" as reward (another version is to use the prompt "is this reasoning step helpful?"); (4) task-specific heuristics - for example in Blocksworld they compare the current predicted state of blocks with the goal state to calculate a reward.
    - __Monte Carlo tree search:__ the MCTS planning algorithm is used to explore the space of reasoning trees and find high-reward reasoning traces. It iteratively builds a reasoning tree where nodes represent states and edges represent actions (and thus transitions between states). MCTS maintains a state-action value function $Q : \mathcal{S} \times \mathcal{A} \mapsto \R$, where $Q(s, a)$ is an estimate of the expected future reward of taking action $a$ while in state $s$ and thus assesses the potential of a node by looking ahead and anticipating the future reward considering the trajectories that can be taken from this node.
  
      [add fig 1 overview here]

      Specifically, MCTS planning performs 4 operations at each iteration to expand the tree and update the values of $Q$. These are: 
      1) selection - where the most promising portion of the existing tree is selected to be expanded. Starting from the root, that is $s_0$, the algorithm selects a child node as next node at each level of the tree, ending when a leaf node is reached. The Upper Confidence bounds applied to Trees (UCT) algorithm is used to select child nodes in an effort to compromise between exploration (of less-visited nodes) and exploitation (of high-value nodes).
      2) expansion - where the tree is expanded by adding new child nodes to the leaf node chosen during the selection process. Given the leaf node state, the LLM agent samples $d$ possible actions and the LLM world model then predicts the next state for each of these, resulting in $d$ new child nodes. From these child nodes the largest local reward one is chosen for simulation. (note: if the leaf node is already a terminal state, we go straight to back-propagation).
      3) simulation - where, starting from the node chosen above after expansion, an action is sampled from a roll-out policy (they use the same method as in the expansion phase, generating $d$ candidate actions and choosing the one with highest local reward) and the LLM world model is used to predict the next state. This happens for each new state $s$ and stops when a terminal state is reached. 
      4) back-propagation - where, after reaching a terminal state, we backpropagate the reward on the reasoning path between the root and the terminal state, updating the Q value of each state-action pair along this path by aggregating the rewards in all future steps of node $s$.

      [add fig 3 MCTS planning here]

- Results

  They evaluate on several tasks:

  [insert RAP tasks img here]

  - Plan generation with Blocksworld

    In this task the agent aims to produce a sequence of action to rearrange blocks into stacks in a particular order that is given as a goal.

    [insert table 1 and fig 4 Blocksworld results here]

  - Maths reasoning with GSM-8k

    In this task the agent aims to perform calculations in a sequence to arrive at the answer of the given question. 

    [insert table 2 and fig 5 maths results here]

  - Logical reasoning with PrOntoQA
    
    In this task a set of facts and logical rules is provided, and the agent should verify if a given hypothesis is true or false by applying the logical rules to the facts. In this task, a state is defined as the fact which the agent is currently focusing on, and an action is the selection of a logical rule.

    [insert table 3 and fig 6 logic results here]